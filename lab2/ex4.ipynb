{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b02754b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from pyod.models.lof import LOF\n",
    "from pyod.models.combination import average, maximization\n",
    "from pyod.utils.utility import standardizer\n",
    "from scipy.io import loadmat\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ce72371",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Balanced Accuracy ###\n",
    "\n",
    "def confusion_stats(y_true, y_pred):\n",
    "    y_true = np.array(y_true).ravel()\n",
    "    y_pred = np.array(y_pred).ravel()\n",
    "    TP = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    TN = np.sum((y_true == 0) & (y_pred == 0))\n",
    "    FP = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    FN = np.sum((y_true == 1) & (y_pred == 0))\n",
    "    return TP, TN, FP, FN\n",
    "\n",
    "def metrics_from_confusion(TP, TN, FP, FN):\n",
    "    recall = TP / (TP + FN) if (TP + FN) > 0 else 0.0\n",
    "    specificity = TN / (TN + FP) if (TN + FP) > 0 else 0.0\n",
    "    balanced_accuracy = 0.5 * (recall + specificity)\n",
    "    return balanced_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "77a448b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "\n",
    "cardio_data = loadmat('cardio.mat')\n",
    "\n",
    "X = cardio_data['X']\n",
    "y = cardio_data['y'].ravel()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size = 0.25, random_state = 42, stratify = y\n",
    ")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "# scaler = RobustScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "3e70771c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_scores.shape: (1373, 30)\n",
      "test_scores.shape: (458, 30)\n"
     ]
    }
   ],
   "source": [
    "### now onto the ensemble itself ###\n",
    "contamination = np.sum(y_train == 1) / np.size(y_train)\n",
    "k_neighbors = range(30, 120, 3)\n",
    "\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "\n",
    "for i, neighbors in enumerate(k_neighbors):\n",
    "    lof = LOF(n_neighbors=neighbors, contamination=contamination)\n",
    "    lof.fit(X_train)\n",
    "\n",
    "    y_train_pred = lof.predict(X_train)\n",
    "    y_test_pred = lof.predict(X_test)\n",
    "\n",
    "    # TP_train, TN_train, FP_train, FN_train = confusion_stats(y_train, y_train_pred)\n",
    "    # TP_test, TN_test, FP_test, FN_test = confusion_stats(y_test, y_test_pred)\n",
    "    # balanced_accuracy_train = metrics_from_confusion(TP_train, TN_train, FP_train, FN_train)\n",
    "    # balanced_accuracy_test = metrics_from_confusion(TP_test, TN_test, FP_test, FN_test)\n",
    "\n",
    "    decision_fun_train = lof.decision_function(X_train)\n",
    "    decision_fun_test = lof.decision_function(X_test)\n",
    "\n",
    "    train_scores.append(decision_fun_train)\n",
    "    test_scores.append(decision_fun_test)\n",
    "\n",
    "train_scores = np.vstack(train_scores)\n",
    "test_scores  = np.vstack(test_scores)\n",
    "train_scores = train_scores.T\n",
    "test_scores  = test_scores.T\n",
    "\n",
    "\n",
    "print(\"train_scores.shape:\", train_scores.shape)\n",
    "print(\"test_scores.shape:\", test_scores.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "11dca6be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true outliers (test): 44 pred outliers (test): 31\n",
      "TP,TN,FP,FN: 8 391 23 36\n",
      "balanced accuracy (test): 0.5631313131313131\n"
     ]
    }
   ],
   "source": [
    "### AVERAGE METHOD ###\n",
    "\n",
    "train_scores_s = standardizer(train_scores)\n",
    "test_scores_s  = standardizer(test_scores)\n",
    "train_avg = average(train_scores_s)\n",
    "test_avg  = average(test_scores_s)\n",
    "\n",
    "# print(train_avg)\n",
    "# print(test_avg)\n",
    "\n",
    "thr = np.quantile(train_avg, 1-contamination)\n",
    "y_train_ens = (train_avg >= thr).astype(int)\n",
    "y_test_ens  = (test_avg  >= thr).astype(int)\n",
    "\n",
    "print(\"true outliers (test):\", int(y_test.sum()), \"pred outliers (test):\", int(y_test_ens.sum()))\n",
    "TP, TN, FP, FN = confusion_stats(y_test, y_test_ens)\n",
    "print(\"TP,TN,FP,FN:\", TP, TN, FP, FN)\n",
    "print(\"balanced accuracy (test):\", metrics_from_confusion(TP, TN, FP, FN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "fdab96f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true outliers (test): 44 pred outliers (test): 31\n",
      "TP,TN,FP,FN: 10 393 21 34\n",
      "balanced accuracy (test): 0.5882740447957839\n"
     ]
    }
   ],
   "source": [
    "### MAXIMIZATION METHOD ###\n",
    "\n",
    "train_max = maximization(train_scores_s)\n",
    "test_max  = maximization(test_scores_s)\n",
    "\n",
    "# print(train_avg)\n",
    "# print(test_avg)\n",
    "\n",
    "thr = np.quantile(train_max, 1-contamination)\n",
    "y_train_ens = (train_max >= thr).astype(int)\n",
    "y_test_ens  = (test_max  >= thr).astype(int)\n",
    "\n",
    "print(\"true outliers (test):\", int(y_test.sum()), \"pred outliers (test):\", int(y_test_ens.sum()))\n",
    "TP, TN, FP, FN = confusion_stats(y_test, y_test_ens)\n",
    "print(\"TP,TN,FP,FN:\", TP, TN, FP, FN)\n",
    "print(\"balanced accuracy (test):\", metrics_from_confusion(TP, TN, FP, FN))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b272b6",
   "metadata": {},
   "source": [
    "Final observations:\n",
    "- Used LOF ensemble method\n",
    "- Maximization is best when StandardScaler is used, but Average becomes best if RobustScaler is used"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AD",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
